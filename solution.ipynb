{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90576a8d",
   "metadata": {},
   "source": [
    "Скачаем датасет с короткими постами sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdfa0cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://code.s3.yandex.net/deep-learning/tweets.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"data/raw_dataset.csv\", \"wb\") as f:\n",
    "    f.write(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a103ffc",
   "metadata": {},
   "source": [
    "В data_utils.py\n",
    "- удалим ссылкина сайты\n",
    "- приведем к нижнем регистру\n",
    "- удалим все, кроме латинских букв, цифр и пробелов\n",
    "- удалим дублирующиеся пробелы, пробелы по краям\n",
    "- токенизируем текст\n",
    "- разобьем на 3 датасета: тренировочный, валидационный, тестовый"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4193669",
   "metadata": {},
   "source": [
    "\"очистим\" текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.data_utils\n",
    "\n",
    "importlib.reload(src.data_utils)\n",
    "\n",
    "from src.data_utils import preprocess_dataset\n",
    "\n",
    "preprocess_dataset(\"data/raw_dataset.csv\", \"data/dataset_processed.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f15a3e6",
   "metadata": {},
   "source": [
    "токенизируем текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d5efd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.data_utils\n",
    "\n",
    "importlib.reload(src.data_utils)\n",
    "\n",
    "\n",
    "from src.data_utils import tokenize_dataset\n",
    "\n",
    "tokenize_dataset(\n",
    "    \"data/dataset_processed.csv\",\n",
    "    \"data/tokenized_dataset.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf71a667",
   "metadata": {},
   "source": [
    "Разобьём на 3 датасета: тренировочный (80%), валидационный (10%), тестовый (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7258ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.data_utils\n",
    "\n",
    "importlib.reload(src.data_utils)\n",
    "\n",
    "from src.data_utils import split_dataset\n",
    "\n",
    "split_dataset(\n",
    "    input_path=\"data/tokenized_dataset.csv\",\n",
    "    train_path=\"data/train.csv\",\n",
    "    val_path=\"data/val.csv\",\n",
    "    test_path=\"data/test.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf56dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "В next_token_dataset.py\n",
    "Создадим даталоадеры и train.csv, val.csv, test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d75647c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batches: 20015\n",
      "val batches: 2502\n",
      "test batches: 2502\n"
     ]
    }
   ],
   "source": [
    "from src.next_token_dataset import make_dataloader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pad_id = tokenizer.pad_token_id  # у bert-base-uncased это 0\n",
    "\n",
    "batch_size = 64\n",
    "max_len = 128\n",
    "\n",
    "_, train_loader = make_dataloader(\"data/train.csv\", batch_size=batch_size, shuffle=True,  max_len=max_len, pad_id=pad_id)\n",
    "_, val_loader   = make_dataloader(\"data/val.csv\",   batch_size=batch_size, shuffle=False, max_len=max_len, pad_id=pad_id)\n",
    "_, test_loader  = make_dataloader(\"data/test.csv\",  batch_size=batch_size, shuffle=False, max_len=max_len, pad_id=pad_id)\n",
    "\n",
    "print(\"train batches:\", len(train_loader))\n",
    "print(\"val batches:\", len(val_loader))\n",
    "print(\"test batches:\", len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28684e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([64, 31])\n",
      "targets: torch.Size([64, 31])\n",
      "lengths: torch.Size([64])\n",
      "X tokens: ['[CLS]', 'my', '##sw', '##eet', '##mel', '##od', '##y', 'nice', 'tie', 'i', 'love', 'the', 'green']\n",
      "Y tokens: ['my', '##sw', '##eet', '##mel', '##od', '##y', 'nice', 'tie', 'i', 'love', 'the', 'green', '[SEP]']\n",
      "Shift OK: True\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"input_ids:\", batch[\"input_ids\"].shape)\n",
    "print(\"targets:\", batch[\"targets\"].shape)\n",
    "print(\"lengths:\", batch[\"lengths\"].shape)\n",
    "\n",
    "# посмотрим на один пример в батче (без паддинга)\n",
    "i = 0\n",
    "L = batch[\"lengths\"][i].item()\n",
    "\n",
    "x_ids = batch[\"input_ids\"][i, :L].tolist()\n",
    "y_ids = batch[\"targets\"][i, :L].tolist()\n",
    "\n",
    "print(\"X tokens:\", tokenizer.convert_ids_to_tokens(x_ids[:20]))\n",
    "print(\"Y tokens:\", tokenizer.convert_ids_to_tokens(y_ids[:20]))\n",
    "\n",
    "# sanity-check: Y должен быть X, сдвинутым на 1\n",
    "print(\"Shift OK:\", x_ids[1:] == y_ids[:-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
